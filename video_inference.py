import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
import argparse
import cv2
import numpy as np
import time
from model.flol import create_model
from options.options import parse
from ultralytics import YOLO  # <--- [1] [THÃŠM] Import thÆ° viá»‡n YOLO

def pad_tensor(tensor, multiple=8):
    _, _, H, W = tensor.shape
    pad_h = (multiple - H % multiple) % multiple
    pad_w = (multiple - W % multiple) % multiple
    tensor = F.pad(tensor, (0, pad_w, 0, pad_h), value=0)
    return tensor

# [Sá»¬A] ThÃªm tham sá»‘ yolo_path vÃ o hÃ m main
def main(opt, input_path, output_path, scale_percent, yolo_path):
    # 1. Cáº¥u hÃ¬nh thiáº¿t bá»‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"--- Äang cháº¡y trÃªn thiáº¿t bá»‹: {device} ---")

    # 2. Load Model FLOL
    print("â³ Äang táº£i mÃ´ hÃ¬nh FLOL (LÃ m sÃ¡ng)...")
    model = create_model()
    weights_path = opt['settings']['weight']
    checkpoint = torch.load(weights_path, map_location=device)
    model.load_state_dict(checkpoint['params'])
    model.to(device)
    model.eval()
    print("âœ… ÄÃ£ táº£i FLOL thÃ nh cÃ´ng!")

    # --- [2] [THÃŠM] LOAD MODEL YOLO ---
    print(f"â³ Äang táº£i mÃ´ hÃ¬nh YOLO tá»«: {yolo_path} ...")
    try:
        yolo_model = YOLO(yolo_path)
        print("âœ… ÄÃ£ táº£i YOLO thÃ nh cÃ´ng!")
    except Exception as e:
        print(f"âŒ Lá»—i táº£i YOLO: {e}")
        return
    # ----------------------------------

    # 3. Má»Ÿ Video
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        print(f"Lá»—i: KhÃ´ng má»Ÿ Ä‘Æ°á»£c video {input_path}")
        return

    # Láº¥y thÃ´ng sá»‘ gá»‘c
    org_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    org_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if scale_percent < 100:
        new_width = int(org_width * scale_percent / 100)
        new_height = int(org_height * scale_percent / 100)
        print(f"Äang RESIZE video: {org_width}x{org_height} -> {new_width}x{new_height}")
    else:
        new_width = org_width
        new_height = org_height
        print(f"Giá»¯ nguyÃªn Ä‘á»™ phÃ¢n giáº£i: {org_width}x{org_height}")

    # 4. Video Writer 
    fourcc = cv2.VideoWriter_fourcc(*'mp4v') 
    out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))

    to_tensor = transforms.ToTensor()
    frame_count = 0
    start_time = time.time()

    print("ğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½ Combo FLOL + YOLO... (Nháº¥n 'q' Ä‘á»ƒ dá»«ng sá»›m)")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # --- RESIZE ---
        if scale_percent < 100:
            frame_processing = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_AREA)
        else:
            frame_processing = frame

        # --- Xá»¬ LÃ FLOL (LÃ€M SÃNG) ---
        img_rgb = cv2.cvtColor(frame_processing, cv2.COLOR_BGR2RGB)
        img_tensor = to_tensor(img_rgb).unsqueeze(0).to(device)
        
        # Padding 
        _, _, H, W = img_tensor.shape
        img_padded = pad_tensor(img_tensor)

        with torch.no_grad():
            output = model(img_padded)

        # Háº­u xá»­ lÃ½ FLOL -> Ra áº£nh sÃ¡ng (output_bgr)
        output = torch.clamp(output, 0., 1.)
        output = output[:, :, :H, :W] # Cáº¯t bá» pháº§n padding
        output_np = output.squeeze(0).permute(1, 2, 0).cpu().numpy()
        
        output_bgr = (output_np * 255).astype(np.uint8)
        output_bgr = cv2.cvtColor(output_bgr, cv2.COLOR_RGB2BGR)

        # --- [3] [THÃŠM] CHáº Y YOLO NHáº¬N DIá»†N ---
        # Láº¥y áº£nh Ä‘Ã£ lÃ m sÃ¡ng (output_bgr) Ä‘Æ°a vÃ o YOLO
        # conf=0.4: Chá»‰ hiá»‡n khung náº¿u Ä‘á»™ tin cáº­y > 40%
        results = yolo_model(output_bgr, verbose=False, conf=0.4)
        
        # Láº¥y áº£nh Ä‘Ã£ Ä‘Æ°á»£c váº½ khung nháº­n diá»‡n (Annotated Frame)
        final_frame = results[0].plot()
        # ---------------------------------------

        # [Sá»¬A] Ghi áº£nh cuá»‘i cÃ¹ng (Ä‘Ã£ cÃ³ khung) vÃ o video
        out.write(final_frame)

        frame_count += 1
        if frame_count % 10 == 0: 
            elapsed = time.time() - start_time
            process_fps = frame_count / elapsed
            print(f"\rTiáº¿n Ä‘á»™: {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%) | Tá»‘c Ä‘á»™: {process_fps:.1f} FPS", end="")

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()
    print(f"\n\nâœ… Xong! Video Ä‘Ã£ lÆ°u táº¡i: {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="./options/LOLv2-Real.yml")
    
    parser.add_argument("--input", type=str, 
                        default="datasets/LOLv2-Real/test/Low/test6.mp4", 
                        help="ÄÆ°á»ng dáº«n file video Ä‘áº§u vÃ o")

    parser.add_argument("--output", type=str, 
                        default="results/LOLv2-Real/test6_result.mp4", 
                        help="ÄÆ°á»ng dáº«n file video káº¿t quáº£")

    parser.add_argument("--scale", type=int, default=50, help="Tá»· lá»‡ % resize")

    # [THÃŠM] Tham sá»‘ Ä‘Æ°á»ng dáº«n file best.pt
    parser.add_argument("--yolo", type=str, default="yolo11n.pt", help="ÄÆ°á»ng dáº«n file trá»ng sá»‘ YOLO")
    
    args = parser.parse_args()
    opt = parse(args.config)

    import os
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ÄÃ£ tá»± Ä‘á»™ng táº¡o thÆ° má»¥c: {output_dir}")

    # [Sá»¬A] Truyá»n thÃªm tham sá»‘ args.yolo
    main(opt, args.input, args.output, args.scale, args.yolo)